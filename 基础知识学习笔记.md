# 基础知识学习笔记

## 2025.12.10

- 基础代码方便

1. 关于 python中的 protocol基类，只要内部结构包含（兼容）与继承 protocol的类相似，便可以任务是这种类型

- [ ] `Embedding-Model`的获取

  - 方法一：使用云端接口访问
  - 方法二：使用模型
- [ ] `python`的多继承
- [ ] `python`的异步实现以及相应的优化

## 2025.12.11

- pythno中的异步实现：为了处理高并发任务

1. 明确进程，线程，协程之间的关系，如何进行理解

### 异步实现

> 核心关键字：
>
> - asyncio：线程实现异步的包
> - async：创建异步线程函数的关键字，用于定义协程函数
> - await，用于等待异步操作的完成
> - asynci.run(),运行异步主函数，否则输出一个协程
> - asyncio.gather(),并发执行多个协程，同时执行多个协程

### 调用大模型的异步实现

## 2025.11.12

关于长文本或多文件的 `map-reduce`处理，结合分布式操作进行

## 异步编程

> 1. 如何理解事件循环
> 2. 让出事件循环，还会不会在后台运行
> 3. 真阻塞（例如 time.sleep如何包装成可以异步进行，用线程来包装吗？）
> 4. 包装成线程，如何进行理解？

**注意事项**

1. 对于可等待的对象（coroutine）,只有获取结果后，才能访问其属性
2. 对于占用耗时本身计算的任务，让其等待只会卡死。对于在服务端进行计算的任务（例如网络请求后获取响应），让其等待，本质上不会占用本地的 cpu,只是在服务端进行
3. 包装成新线程，就是再请一个工人给你干活，我去做其他事情
4. 对于单任务，使用同步和异步都可以，只是异步在运行时会额外都一个调度层，这是耗时上的主要区别，并且异步可以容忍多任务，相比单任务，处理并发更有优势，除了简单脚本的运行，都推荐使用异步编程

补充一点：

> 本地计算任务（cpu密集）与数据传输任务（IO密集）

创建一个异步函数的情况：

1. 基础函数本身就是 coroutine,的，只需要使用 await即可
2. 基础函数是同步的耗时的，可以使用 async.to_thread(funcation,arg),将其包装为协程，变成情况1

### 并发实现

### 并发限流

## 大模型调用

> `Agent`的开发与 `RAG`中不可避免的要使用大模型。这里调用大模型结合 `<font color=red>`定制化 `</font>`的 `prompt`进行输出

### 大模型的耗时理解

> `OpenAI API`的耗时，本质上 `IO	`堵塞等待网络响应。其耗时有三部分：
>
> - 第一步：用户端发起请求，服务端接收请求或进行排队耗时。耗时波动很大
> - 第二步：模型的计算耗时（在服务端进行计算推理耗时，最大）
> - 第三步：结果通过网络进行传输的耗时（这段时间用户端只是在等待服务端的响应，较小）

- 显然在不启用 stream式输出的话，必须等在服务端完成全部的推理后才能接受到。数据进出（IO）耗时很大

那有时候又使用openai的asyncopenai,创建一个异步客户端，这个是什么，同步客户端如何创建，他们有什么区别呢

在使用OpenAi框架进行调用大模型框架生成时，它这个耗时是类似sleep,还是async.sleep，为什么，如果一个函数使用的是sleep,是否可以将其包装为协程‘’

### 调用原理分析

**两种调用**

- 使用 langchain_openai.ChatOpenAI(base_url,api_key,model)(prompt)进行输出
  1. 这个函数是同步，还是异步？如果是同步，如何包装为异步
- 使用 openai.AsyncioOpenAI(base_url,api_key).chat.completions.creat(model,message,……).

各种调用以及调用特点分析

| 方式               | 是否阻塞 | 是否真 async | 并发能力 | 推荐度     |
| ------------------ | -------- | ------------ | -------- | ---------- |
| LangChain invoke   | ✅       | ❌           | ❌       | ⭐         |
| LangChain ainvoke  | ❌       | ✅           | ✅       | ⭐⭐⭐⭐   |
| OpenAI AsyncOpenAI | ❌       | ✅           | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| asyncio.to_thread  | ❌       | ⚠️ 半真    | ⚠️     | ⭐⭐       |

#### 使用langchain封装好的框架进行

- 同步调用： langchain_openai.ChatOpenAI(base_url,api_key,model).invoke(prompt).content
- 

#### 使用OpenAI原生SDK进行

当前的一些问题：

1. 对于多轮对话，只需创建一个客户端即可
2. 对于并发提问，需要创建多个客户端
3. async with 是什么含义，如何使用，async for 又是什么
4. chat.completion与responses在处理stream上有什么区别
5. responses使用工具，如何高效理解
6. 多轮提问（预设好问题）是否可以看到前面的，但只是一个messgage,如何，多轮交流，这个怎么使用responses框架进行完成
7. 比如本地模型进行计算，比如多个文本过本地模型，输出是否。可以使用并发实现
8. with,上下文管理

### 常用框架

## Langchain/LangGraph代码

### LLM大模型初始代码

#### 初始化代码

> 一般是调用大模型的API进行操作

以下是基于 `langchain`调用大模型

```python
from langchain_openai import ChatOpenAI

# 建议将 API Key 放在环境变量中，或者直接在此处赋值（生产环境请勿硬编码）
# os.environ["DEEPSEEK_API_KEY"] = "sk-你的DeepSeek密钥"

# 初始化模型
llm = ChatOpenAI(
    model="deepseek-chat",             # DeepSeek V3 模型名称
    openai_api_key="sk-你的Key",        # 填入你的 DeepSeek API Key
    openai_api_base="https://api.deepseek.com", # DeepSeek 的 API 地址
    temperature=0.7,
    max_tokens=1024
)

```

#### 简单的调用

> 让大模型接受输入，并获取其回应，里面是一个消息队列（Human,AI,Tool,System）

```python
response = llm.invoke([HumanMessage(content=)])

```

1. 有时也会 llm.invoke("解释 MapReduce")，这是一个语法糖的简化形式，其内部等价与  llm.invoke([HumanMessage(content="解释 MapReduce")])

### Message

> 该部分继承（`langchain_core.messages.BaseMessage=[HumanMesaage(content))]`）,不只是文本，还包含角色，文本，工具调用信息

```plaintext
BaseMessage
 ├── HumanMessage   # 用户输入
 ├── AIMessage      # 模型输出
 ├── ToolMessage    # 工具返回
 └── SystemMessage  # system prompt

```

- humanMessage=HumanMessage(content=),用户提问
- 

## LangGraph

> 1. LangGraph = [状态机+LLM节点+条件跳转]
> 2. State是全局上下文
> 3. 所有的node都是：State->Partial[State]
> 4. 构建 graph,
> 5. 执行

### 基础用法

1. 定义全局的 State
2. 定义节点 Node，Node不能修改原来的 state，只做增量处理

**潜在问题**

> 1. langchain_core.messages.BaseMessage与langchain_core.messages.HumanMessage是什么？如何使用？
> 2. TypeDict与普通的Dict有什么区别
> 3. graph.add_node,graph.set_entry_point,graph.add_edge，graph.compile这是什么？如何使用？
> 4. state["messages"][-1].content，为什么是 [-1],state["message"]返回的是个什么内容呢？
> 5. graph.add_conditional_edges是什么？如何使用？


**常见代码含义**

- `graph=StateGraph(State)`:创建一个图
- `graph.add_node("chat":str,caht_node:funcation)`:图里有一个节点，名字叫 `"chat"`，执行函数是 `chat_node`
- `graph.set_entry_point("chat")`:该图流程从哪个节点开始
- `add_edge(node_name:str,node_name:str)`:节点直接的跳转
- `app=graph.compile()`:,检验graph是否合法，构建执行引擎，生成可执行对象
- `graph.add_conditional_edges(node_name:str,should_continue:funcation)`:从节点 `node_name`,开始执行判断函数 `should_continue`
