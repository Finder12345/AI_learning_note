# 论文阅读笔记

## GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graph Question Answering

### 原文核心

> 知识图谱问答中的检索增强微调：（GraphRAFT = Graph + Retrieval Augmented Fine-Tuning）

- 目标问题：如何让大模型在“图数据库 / 知识图谱”场景下，稳定、可控地生成正确的结构化查询（如 Cypher），而不是靠 Prompt 硬猜？
- 传统的NL2Cypher问题：

  1. Schema变化，效果就变差：模型对schema的理解很浅，单纯的模式匹配，而非结构理解
  2. 大模型微调的问题：schema与Cypher模式过拟合，训练中的Schema的结构被硬编码到模型权重中，导致无法适应新的Schema结构
- 核心思想：不是“用 RAG 辅助推理”，而是“用 RAG 来训练模型如何用图结构”
- 整体架构：

  ```mathematica
  自然语言问题
     ↓
  Graph-aware Retrieval（子图 / schema 检索）
     ↓
  Retrieval-Augmented Prompt
     ↓
  Fine-tuned LLM
     ↓
  Cypher / 图查询

  ```
- 创新点：

  1. 图感知检索（Graph-aware Retrieval）：检索到子图（并非传统的检索到文本）
     1. 检索（节点、属性，关系路径）->task-Specific sub_graph
     2. 避免：全图太大导致窗口限制，无关结构干扰推理
     3. 意义：减少搜索空间、提供结构先验、全局图转化为局部图
     4. 大致步骤：实体识别，检索子图，在子图上进行推理
  2. 子图进行线性化（Graph->Text）：
  3. 检索增强微调（Retrieval-Augmented Fine-Tuning）：通过检索到相关子图进行生产Cypher
     1. 传统：问题+全量的Schema->Cypher
     2. RAFT：问题+检索到的相关子图（sub_sche）->cypher
     3. 让模型学习，拿到一个子图，如何基于它写查询Cypher
  4. 减少幻觉：基于子图，避免不存在的关系，不存在的节点，让模型只能关注被检索的子图结构


### 理解启发或问题

1. 关于图感知检索技术：感知子图的意义是什么？如何实现？原图与子图的表征形式是什么？文中提交的检索子图与子模版（sub_schema）它们是不是一个东西？是否可以结合原Schema，类似的生成Sub_Schema，这个思路与现在的Schema动态裁剪有什么区别（二者的本质都是获取子图/sub_schema）?
2. 关于子图线性化：感知子图线性化的意义是什么？难道还只是使用text记录节点的形式进行表征吗？
3. 关于RAFT：核心在训练大模型的基于图进行查询语句生成的能力，这样换不同的KG，也可以有不错的泛化能力
4. 关于只做RAG，检索到 schema，但模型不会“用”，什么叫检索到Schema?在图查询中，不是生成Cypher，然后在图数据库中检索，用于后续的生成吗？文中的检索Schema是什么，这还是RAG吗，这就是KG的图感知检索吧（生成所谓的sub_schema）?
5. 现在使用大模型的api进行（不做预训练），是否可以利用大模型本身检索子图或者（sub_schema）的能力进行实现？如果可以检索到的子图如何保证可以覆盖查询？
6. KG中的Sub_graph与Sub_Schema，前者的实现可以通过Cypher引擎进行操作（其结构是实例化的，是一定存在的），而后者的是如何实现的，貌似不一定存在，它们的效果对比一下
